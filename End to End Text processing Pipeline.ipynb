{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read/create the text data\n",
    "# Letâ€™s create a list of strings and assign it to a variable. \n",
    "# Maybe a tweet sample:\n",
    "tweet_sample= \"How to take control of your #debt https://personal.vanguard.com/us/insights/saving-investing/debt-management. #Best advice for #family #financial #success (@PrepareToWin)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to take control of your debt URL Best advice for family financial success AT_USER'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Execute the below function to process the tweet:\n",
    "def processRow(row):\n",
    " import re\n",
    " import nltk\n",
    " from textblob import TextBlob\n",
    " from nltk.corpus import stopwords\n",
    " from nltk.stem import PorterStemmer\n",
    " from textblob import Word\n",
    " from nltk.util import ngrams\n",
    " from nltk.tokenize import word_tokenize\n",
    " \n",
    " tweet = row\n",
    "\n",
    "#Lower case\n",
    " tweet.lower()\n",
    "\n",
    "#Removes unicode strings like \"\\u002c\"  -> ,(comma)\n",
    " tweet = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', tweet)\n",
    "    \n",
    "# Removes non-ascii characters. note : \\x00 to \\x7f is 00 to 255\n",
    "# non-ascii characters like copyrigth symbol, trademark symbol\n",
    " tweet = re.sub(r'[^\\x00-\\x7f]',r'',tweet)\n",
    "               \n",
    "#convert any url to URL\n",
    " tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "               \n",
    "#Convert any @Username to \"AT_USER\"\n",
    " tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "\n",
    "#Replace multiple white spaces with one white space\n",
    " tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "#Replace multiple break-line i.e enter-key,  with one white space\n",
    " tweet = re.sub('[\\n]+', ' ', tweet)\n",
    "\n",
    "# Removes hastag in front of a word\n",
    "# or simply said Replace #word with word\n",
    " tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "#Replace non-word chars with a single white space\n",
    " tweet = re.sub(r'[^\\w]', ' ', tweet)\n",
    "  \n",
    "# Remove smiley face symbols\n",
    " tweet = tweet.replace(':)','')\n",
    " tweet = tweet.replace(':(','')\n",
    "# below we have removed all possible emoticons / smiley faces\n",
    "\n",
    "#Removes emoticons from text\n",
    " tweet = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', tweet)\n",
    "\n",
    "#remove numbers\n",
    " tweet = ''.join([i for i in tweet if not i.isdigit()])\n",
    "\n",
    "#remove multiple exclamation  -> this is optional\n",
    " tweet = re.sub(r\"(\\!)\\1+\", ' ', tweet)\n",
    "\n",
    "#remove multiple question marks -> this is optional\n",
    " tweet = re.sub(r\"(\\?)\\1+\", ' ', tweet)\n",
    "\n",
    "#remove multistop -> this is optional\n",
    " tweet = re.sub(r\"(\\.)\\1+\", ' ', tweet)\n",
    "\n",
    "#trim -> this is optional, as this would have been removed \n",
    "# by the above [^\\w] step\n",
    " tweet = tweet.strip('\\'\"')\n",
    "\n",
    "# making lemma of the cleaned word\n",
    " from textblob import Word\n",
    " tweet =\" \".join([Word(word).lemmatize() for word in tweet.split()])\n",
    "\n",
    " row = tweet\n",
    " return row\n",
    "               \n",
    "# call the function with your data\n",
    "processRow(tweet_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : The above text pre-processing (*`cleaning`*) function totally depends on type / nature of data. One has to accordingly code his function. Above is just an example to clean tweet data. Depending on the kind of cleaning you plan on the data the above function can go from simple to complex coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the modules / libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Training Data set\n",
    "train = pd.read_csv(\"quora_train_set.csv\", index_col='id')[:1000]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any null values\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some of the pairs of questions\n",
    "a = 0 \n",
    "for i in range(a,a+10):\n",
    "    print(train.question1[i])\n",
    "    print(train.question2[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have defined our own list of Stop words\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, \n",
    "    # with the option to remove stop_words \n",
    "    # and to stem words.\n",
    "\n",
    "    # Clean the text in your own way.  This gives better results. \n",
    "    # So instead of using only regex, we have identified the \n",
    "    # short forms people type & would use in english\n",
    "    # and replaced them.\n",
    "    # This list could go even bigger.\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    # Please note , we could have also generated lemma. \n",
    "    # from textblob import Word\n",
    "    # text = \" \".join([Word(word).lemmatize() for word in text.split()])\n",
    "\n",
    "       \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the above user defined function\n",
    "def process_questions(question_list, questions, \n",
    "                      question_list_name, \n",
    "                      dataframe):\n",
    "    '''transform questions and display progress'''\n",
    "    for question in questions:\n",
    "        question_list.append(text_to_wordlist(question))\n",
    "        if len(question_list) % 100 == 0:\n",
    "            progress = len(question_list)/len(dataframe) * 100\n",
    "            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a empty list to hold the cleaned Question\n",
    "train_question1 = []\n",
    "\n",
    "# calling the above user defn fn.\n",
    "process_questions(train_question1, train.question1, \n",
    "                  'train_question1', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a empty list to hold the cleaned Question\n",
    "train_question2 = []\n",
    "\n",
    "# calling the above user defn fn.\n",
    "process_questions(train_question2, train.question2, \n",
    "                  'train_question2', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some transformed pairs of questions\n",
    "a = 0 \n",
    "for i in range(a,a+10):\n",
    "    print(train_question1[i])\n",
    "    print(train_question2[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
